{
	"jobConfig": {
		"name": "raw_to_silver",
		"description": "",
		"role": "arn:aws:iam::745412505519:role/service-role/AWSGlueServiceRole",
		"command": "pythonshell",
		"version": "3.0",
		"runtime": null,
		"workerType": null,
		"numberOfWorkers": null,
		"maxCapacity": 0.0625,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "raw_to_silver.py",
		"scriptLocation": "s3://aws-glue-assets-745412505519-eu-central-1/scripts/",
		"language": "python-3.9",
		"spark": false,
		"jobParameters": [
			{
				"key": "--JobName",
				"value": "bitcoin-raw-to-silver",
				"existing": false
			},
			{
				"key": "--price_file_name",
				"value": "BTC_EUR_Historical_Data.csv",
				"existing": false
			},
			{
				"key": "--trend_file_name",
				"value": "google_trend_bitcoin.csv",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-06-11T12:56:03.661Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-745412505519-eu-central-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"pythonShellPrebuiltLibraryOption": "analytics",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"script": "import sys\nimport boto3\nfrom awsglue.utils import getResolvedOptions\nimport pandas as pd\nimport awswrangler as wr\nimport os\n\nargs = getResolvedOptions(sys.argv,\n                          ['JobName',\n                           'price_file_name',\n                           'trend_file_name'\n                           ])\n             \njob_name = args['JobName']\nprice_file_name = args['price_file_name']\ntrend_file_name = args['trend_file_name']\n\nprint(f\"{job_name}: the price_file_name is: {price_file_name}\")\nprint(f\"{job_name}: the trend_file_name is: {trend_file_name}\")\n\ns3 = boto3.resource('s3')\n\n# FIXME: transform these constants below into parameters\nbucketname = \"kotik-profai\"\nsource = \"aws-de/raw\"\ntarget = \"aws-de/silver\"\n\nbucket = s3.Bucket(bucketname)\n\ndef _get_file_name_without_extension(file_name):\n    return os.path.basename(file_name).split('.')[0]\n\nfor fn in [price_file_name, trend_file_name]:\n    try:\n        obj = s3.Object(bucketname, source + '/' + fn)\n    except Exception as e:\n        print(f\"error get object: {e}\")\n        raise e\n    \n    print(f\"{job_name}: loading file {fn}\")\n\n    fn_parquet = _get_file_name_without_extension(fn) + '.parqet'\n    target_filename = f\"s3://{bucketname}/{target}/{fn_parquet}\"\n    \n    print(f\"{job_name} target_filename: {target_filename}\")\n    \n    # Load and (first) transformations\n    if fn == price_file_name:\n        df = pd.read_csv(obj.get()['Body'], parse_dates=['Date'], thousands=',')\n        # Get rid of some columns we are not interested in\n        df = df.drop(axis=1, columns=['Open', 'High', 'Low', 'Vol.', 'Change %'])\n        # Find record with invalid price and fill them with rolling mean\n        df.loc[df['Price'] == -1, 'Price'] = df['Price'].rolling(window=5, min_periods=1).mean()\n    else:\n        df = pd.read_csv(obj.get()['Body'], parse_dates=['Settimana'], thousands=',')\n        # Normalize column names\n        df.columns.values[0] = 'Week'\n        df.columns.values[1] = 'Google_trend'\n\n    wr.s3.to_parquet(\n        df = df,\n        path = target_filename\n    )\n\n    print(f\"{job_name}: about to delete {fn}\")\n# TODO uncomment the below line before final delivery    \n#    obj.delete()\n\n"
}